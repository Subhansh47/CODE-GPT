




















Pipelines for inference






Hugging Face







					Models

					Datasets

					Spaces

					Docs




			Solutions
		

Pricing
			






Log In
				
Sign Up
					




Transformers documentation
			
Pipelines for inference





					Transformers
					



Search documentation


mainv4.34.1v4.33.3v4.32.1v4.31.0v4.30.0v4.29.1v4.28.1v4.27.2v4.26.1v4.25.1v4.24.0v4.23.1v4.22.2v4.21.3v4.20.1v4.19.4v4.18.0v4.17.0v4.16.2v4.15.0v4.14.1v4.13.0v4.12.5v4.11.3v4.10.1v4.9.2v4.8.2v4.7.0v4.6.0v4.5.1v4.4.2v4.3.3v4.2.2v4.1.1v4.0.1v3.5.1v3.4.0v3.3.1v3.2.0v3.1.0v3.0.2v2.11.0v2.10.0v2.9.1v2.8.0v2.7.0v2.6.0v2.5.1v2.4.1v2.3.0v2.2.2v2.1.1v2.0.0v1.2.0v1.1.0v1.0.0doc-builder-html
DEENESFRITKOPTZH








Get started


ü§ó Transformers
Quick tour
Installation


Tutorials


Run inference with pipelines
Write portable code with AutoClass
Preprocess data
Fine-tune a pretrained model
Train with a script
Set up distributed training with ü§ó Accelerate
Load and train adapters with ü§ó PEFT
Share your model
Agents
Generation with LLMs


Task Guides



Natural Language Processing


Audio


Computer Vision


Multimodal


Generation


Prompting



Developer guides


Use fast tokenizers from ü§ó Tokenizers
Run inference with multilingual models
Use model-specific APIs
Share a custom model
Templates for chat models
Run training on Amazon SageMaker
Export to ONNX
Export to TFLite
Export to TorchScript
Benchmarks
Notebooks with examples
Community resources
Custom Tools and Prompts
Troubleshoot


Performance and scalability


Overview

Efficient training techniques


Methods and tools for efficient training on a single GPU
Multiple GPUs and parallelism
Efficient training on CPU
Distributed CPU training
Training on TPUs
Training on TPU with TensorFlow
Training on Specialized Hardware
Custom hardware for training
Hyperparameter Search using Trainer API


Optimizing inference


Inference on CPU
Inference on one GPU
Inference on many GPUs
Inference on Specialized Hardware

Instantiating a big model
Troubleshooting
XLA Integration for TensorFlow Models
Optimize inference using `torch.compile()`


Contribute


How to contribute to transformers?
How to add a model to ü§ó Transformers?
How to convert a ü§ó Transformers model to TensorFlow?
How to add a pipeline to ü§ó Transformers?
Testing
Checks on a Pull Request


Conceptual guides


Philosophy
Glossary
What ü§ó Transformers can do
How ü§ó Transformers solve tasks
The Transformer model family
Summary of the tokenizers
Attention mechanisms
Padding and truncation
BERTology
Perplexity of fixed-length models
Pipelines for webserver inference
Model training anatomy


API



Main Classes


Agents and Tools
Auto Classes
Callbacks
Configuration
Data Collator
Keras callbacks
Logging
Models
Text Generation
ONNX
Optimization
Model outputs
Pipelines
Processors
Quantization
Tokenizer
Trainer
DeepSpeed Integration
Feature Extractor
Image Processor


Models



Text models


Vision models


Audio models


Multimodal models


Reinforcement learning models


Time series models


Graph models



Internal Helpers


Custom Layers and Utilities
Utilities for pipelines
Utilities for Tokenizers
Utilities for Trainer
Utilities for Generation
Utilities for Image Processors
Utilities for Audio processing
General Utilities
Utilities for Time Series





Join the Hugging Face community
and get access to the augmented documentation experience
		

Collaborate on models, datasets and Spaces
				

Faster examples with accelerated inference
				

Switch between documentation themes
				
Sign Up
to get started
 












   Pipelines for inference The pipeline() makes it simple to use any model from the Hub for inference on any language, computer vision, speech, and multimodal tasks. Even if you don‚Äôt have experience with a specific modality or aren‚Äôt familiar with the underlying code behind the models, you can still use them for inference with the pipeline()! This tutorial will teach you to: Use a pipeline() for inference. Use a specific tokenizer or model. Use a pipeline() for audio, vision, and multimodal tasks. Take a look at the pipeline() documentation for a complete list of supported tasks and available parameters.  Pipeline usage While each task has an associated pipeline(), it is simpler to use the general pipeline() abstraction which contains
all the task-specific pipelines. The pipeline() automatically loads a default model and a preprocessing class capable
of inference for your task. Let‚Äôs take the example of using the pipeline() for automatic speech recognition (ASR), or
speech-to-text. Start by creating a pipeline() and specify the inference task:   Copied >>> from transformers import pipeline

>>> transcriber = pipeline(task="automatic-speech-recognition") Pass your input to the pipeline(). In the case of speech recognition, this is an audio input file:   Copied >>> transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': 'I HAVE A DREAM BUT ONE DAY THIS NATION WILL RISE UP LIVE UP THE TRUE MEANING OF ITS TREES'} Not the result you had in mind? Check out some of the most downloaded automatic speech recognition models
on the Hub to see if you can get a better transcription. Let‚Äôs try the Whisper large-v2 model from OpenAI. Whisper was released
2 years later than Wav2Vec2, and was trained on close to 10x more data. As such, it beats Wav2Vec2 on most downstream
benchmarks. It also has the added benefit of predicting punctuation and casing, neither of which are possible with
Wav2Vec2. Let‚Äôs give it a try here to see how it performs:   Copied >>> transcriber = pipeline(model="openai/whisper-large-v2")
>>> transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'} Now this result looks more accurate! For a deep-dive comparison on Wav2Vec2 vs Whisper, refer to the Audio Transformers Course.
We really encourage you to check out the Hub for models in different languages, models specialized in your field, and more.
You can check out and compare model results directly from your browser on the Hub to see if it fits or
handles corner cases better than other ones.
And if you don‚Äôt find a model for your use case, you can always start training your own! If you have several inputs, you can pass your input as a list:   Copied transcriber(
    [
        "https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac",
        "https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac",
    ]
) Pipelines are great for experimentation as switching from one model to another is trivial; however, there are some ways to optimize them for larger workloads than experimentation. See the following guides that dive into iterating over whole datasets or using pipelines in a webserver:
of the docs: Using pipelines on a dataset Using pipelines for a webserver  Parameters pipeline() supports many parameters; some are task specific, and some are general to all pipelines.
In general, you can specify parameters anywhere you want:   Copied transcriber = pipeline(model="openai/whisper-large-v2", my_parameter=1)

out = transcriber(...)  # This will use `my_parameter=1`.
out = transcriber(..., my_parameter=2)  # This will override and use `my_parameter=2`.
out = transcriber(...)  # This will go back to using `my_parameter=1`. Let‚Äôs check out 3 important ones:  Device If you use device=n, the pipeline automatically puts the model on the specified device.
This will work regardless of whether you are using PyTorch or Tensorflow.   Copied transcriber = pipeline(model="openai/whisper-large-v2", device=0) If the model is too large for a single GPU and you are using PyTorch, you can set device_map="auto" to automatically
determine how to load and store the model weights. Using the device_map argument requires the ü§ó Accelerate
package:   Copied pip install --upgrade accelerate The following code automatically loads and stores model weights across devices:   Copied transcriber = pipeline(model="openai/whisper-large-v2", device_map="auto") Note that if  device_map="auto" is passed, there is no need to add the argument device=device when instantiating your pipeline as you may encounter some unexpected behavior!  Batch size By default, pipelines will not batch inference for reasons explained in detail here. The reason is that batching is not necessarily faster, and can actually be quite slower in some cases. But if it works in your use case, you can use:   Copied transcriber = pipeline(model="openai/whisper-large-v2", device=0, batch_size=2)
audio_filenames = [f"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/{i}.flac" for i in range(1, 5)]
texts = transcriber(audio_filenames) This runs the pipeline on the 4 provided audio files, but it will pass them in batches of 2
to the model (which is on a GPU, where batching is more likely to help) without requiring any further code from you.
The output should always match what you would have received without batching. It is only meant as a way to help you get more speed out of a pipeline. Pipelines can also alleviate some of the complexities of batching because, for some pipelines, a single item (like a long audio file) needs to be chunked into multiple parts to be processed by a model. The pipeline performs this chunk batching for you.  Task specific parameters All tasks provide task specific parameters which allow for additional flexibility and options to help you get your job done.
For instance, the transformers.AutomaticSpeechRecognitionPipeline.call() method has a return_timestamps parameter which sounds promising for subtitling videos:   Copied >>> transcriber = pipeline(model="openai/whisper-large-v2", return_timestamps=True)
>>> transcriber("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.', 'chunks': [{'timestamp': (0.0, 11.88), 'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its'}, {'timestamp': (11.88, 12.38), 'text': ' creed.'}]} As you can see, the model inferred the text and also outputted when the various sentences were pronounced. There are many parameters available for each task, so check out each task‚Äôs API reference to see what you can tinker with!
For instance, the AutomaticSpeechRecognitionPipeline has a chunk_length_s parameter which is helpful
for working on really long audio files (for example, subtitling entire movies or hour-long videos) that a model typically
cannot handle on its own:   Copied >>> transcriber = pipeline(model="openai/whisper-large-v2", chunk_length_s=30, return_timestamps=True)
>>> transcriber("https://huggingface.co/datasets/sanchit-gandhi/librispeech_long/resolve/main/audio.wav")
{'text': " Chapter 16. I might have told you of the beginning of this liaison in a few lines, but I wanted you to see every step by which we came.  I, too, agree to whatever Marguerite wished, Marguerite to be unable to live apart from me. It was the day after the evening... If you can‚Äôt find a parameter that would really help you out, feel free to request it!  Using pipelines on a dataset The pipeline can also run inference on a large dataset. The easiest way we recommend doing this is by using an iterator:   Copied def data():
    for i in range(1000):
        yield f"My example {i}"


pipe = pipeline(model="gpt2", device=0)
generated_characters = 0
for out in pipe(data()):
    generated_characters += len(out[0]["generated_text"]) The iterator data() yields each result, and the pipeline automatically
recognizes the input is iterable and will start fetching the data while
it continues to process it on the GPU (this uses DataLoader under the hood).
This is important because you don‚Äôt have to allocate memory for the whole dataset
and you can feed the GPU as fast as possible. Since batching could speed things up, it may be useful to try tuning the batch_size parameter here. The simplest way to iterate over a dataset is to just load one from ü§ó Datasets:   Copied # KeyDataset is a util that will just output the item we're interested in.
from transformers.pipelines.pt_utils import KeyDataset
from datasets import load_dataset

pipe = pipeline(model="hf-internal-testing/tiny-random-wav2vec2", device=0)
dataset = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation[:10]")

for out in pipe(KeyDataset(dataset, "audio")):
    print(out)  Using pipelines for a webserver Creating an inference engine is a complex topic which deserves it's own
page. Link  Vision pipeline Using a pipeline() for vision tasks is practically identical. Specify your task and pass your image to the classifier. The image can be a link, a local path or a base64-encoded image. For example, what species of cat is shown below?    Copied >>> from transformers import pipeline

>>> vision_classifier = pipeline(model="google/vit-base-patch16-224")
>>> preds = vision_classifier(
...     images="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg"
... )
>>> preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
>>> preds
[{'score': 0.4335, 'label': 'lynx, catamount'}, {'score': 0.0348, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.0324, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.0239, 'label': 'Egyptian cat'}, {'score': 0.0229, 'label': 'tiger cat'}]  Text pipeline Using a pipeline() for NLP tasks is practically identical.   Copied >>> from transformers import pipeline

>>> # This model is a `zero-shot-classification` model.
>>> # It will classify text, except you are free to choose any label you might imagine
>>> classifier = pipeline(model="facebook/bart-large-mnli")
>>> classifier(
...     "I have a problem with my iphone that needs to be resolved asap!!",
...     candidate_labels=["urgent", "not urgent", "phone", "tablet", "computer"],
... )
{'sequence': 'I have a problem with my iphone that needs to be resolved asap!!', 'labels': ['urgent', 'phone', 'computer', 'not urgent', 'tablet'], 'scores': [0.504, 0.479, 0.013, 0.003, 0.002]}  Multimodal pipeline The pipeline() supports more than one modality. For example, a visual question answering (VQA) task combines text and image. Feel free to use any image link you like and a question you want to ask about the image. The image can be a URL or a local path to the image. For example, if you use this invoice image:   Copied >>> from transformers import pipeline

>>> vqa = pipeline(model="impira/layoutlm-document-qa")
>>> vqa(
...     image="https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png",
...     question="What is the invoice number?",
... )
[{'score': 0.42515, 'answer': 'us-001', 'start': 16, 'end': 16}] To run the example above you need to have pytesseract installed in addition to ü§ó Transformers:   Copied sudo apt install -y tesseract-ocr
pip install pytesseract  Using pipeline on large models with ü§ó accelerate : You can easily run pipeline on large models using ü§ó accelerate! First make sure you have installed accelerate with pip install accelerate. First load your model using device_map="auto"! We will use facebook/opt-1.3b for our example.   Copied # pip install accelerate
import torch
from transformers import pipeline

pipe = pipeline(model="facebook/opt-1.3b", torch_dtype=torch.bfloat16, device_map="auto")
output = pipe("This is a cool example!", do_sample=True, top_p=0.95) You can also pass 8-bit loaded models if you install bitsandbytes and add the argument load_in_8bit=True   Copied # pip install accelerate bitsandbytes
import torch
from transformers import pipeline

pipe = pipeline(model="facebook/opt-1.3b", device_map="auto", model_kwargs={"load_in_8bit": True})
output = pipe("This is a cool example!", do_sample=True, top_p=0.95) Note that you can replace the checkpoint with any of the Hugging Face model that supports large model loading such as BLOOM! 


‚ÜêInstallation
Write portable code with AutoClass‚Üí

Pipelines for inference
Pipeline usage
Parameters
Device
Batch size
Task specific parameters
Using pipelines on a dataset
Using pipelines for a webserver
Vision pipeline
Text pipeline
Multimodal pipeline
Using pipeline on large models with ü§ó accelerate :










